# Unify and Enhance Job Monitoring for Scraper Error Visibility

## Executive Summary

**Current Grade: B- (75/100)**

We have excellent infrastructure for job monitoring but poor visibility into *why* jobs fail. The tools exist but aren't connected or consistently used. This issue outlines how to unify our monitoring systems and surface actionable error insights, starting with Cinema City and Kino Krakow as pilot scrapers.

**Problem**: We rely on console logs to debug scraper failures, can't easily identify error patterns, and don't know our true success rates across complex job chains.

**Solution**: Connect our existing error tracking, categorization, and monitoring systems into a unified dashboard with phased rollout across all scrapers.

---

## Current State Assessment

### ğŸŸ¢ What's Working Well (Grade: A- / 90%)

#### Infrastructure Layer
1. **JobExecutionSummary Database Schema** â­
   - Flexible JSONB `results` field captures scraper-specific metrics
   - Historical tracking beyond Oban's 7-day retention
   - Comprehensive timing and state tracking
   - Location: `lib/eventasaurus_discovery/job_execution_summaries/job_execution_summary.ex`

2. **ObanTelemetry Handler** â­
   - Captures all job lifecycle events (start, stop, exception)
   - Automatically creates JobExecutionSummary records
   - Merges job.meta + return values into results field
   - Sentry integration for critical failures
   - Location: `lib/eventasaurus_app/monitoring/oban_telemetry.ex`

3. **JobExecutionSummaries Context** â­
   - Rich query functions: metrics, timelines, top workers
   - Silent failure detection logic
   - Scraper name extraction and grouping
   - Per-worker metrics and timelines
   - Location: `lib/eventasaurus_discovery/job_execution_summaries.ex`

4. **ErrorCategories Module** â­
   - Comprehensive 9-category classification system:
     - validation_error, geocoding_error, venue_error
     - performer_error, category_error, duplicate_error
     - network_error, data_quality_error, unknown_error
   - Pattern-based categorization with clear heuristics
   - Location: `lib/eventasaurus_discovery/metrics/error_categories.ex`

5. **MetricsTracker** âœ…
   - Updates Oban.Job.meta with error categorization
   - Records external_id for correlation
   - Location: `lib/eventasaurus_discovery/metrics/metrics_tracker.ex`

6. **Job Execution Monitor UI** âœ…
   - Clean dashboard at `/admin/job-executions`
   - System-wide metrics (success rate, duration)
   - Per-scraper breakdown
   - Individual worker drill-downs
   - Job lineage tracking ("View Tree")
   - Location: `lib/eventasaurus_web/live/admin/job_execution_monitor_live.ex`

### ğŸŸ¡ What Needs Improvement (Grade: C / 70%)

#### Adoption & Consistency Issues

1. **MetricsTracker Usage** âš ï¸
   - **Problem**: Only used in Cinema City and Kino Krakow scrapers
   - **Impact**: No error categorization for 80% of scrapers
   - **Evidence**: `grep -r "MetricsTracker" lib/eventasaurus_discovery/sources` shows 0 results
   - **Root Cause**: No enforcement or documentation requiring its use

2. **Error Category Visibility** âš ï¸
   - **Problem**: Error categories exist but poorly displayed
   - **Current UX**: Buried in long text string in "Results" column
   - **Example**: `"Error category: unknown_error, Error message: Missing icon text for 'pin'"`
   - **User Pain**: Can't filter by error type, can't see patterns

3. **Silent Failure Detection** âš ï¸
   - **Problem**: `detect_silent_failures()` exists but not surfaced in UI
   - **Impact**: Jobs appear successful but created zero entities
   - **Use Case**: Scraper completes but found no events (network issue? site changed?)

4. **Console Log Dependency** âš ï¸
   - **Problem**: Still primary debugging method
   - **Impact**: Difficult to correlate failures across time/scrapers
   - **Evidence**: User mentioned "we still rely on logs in the console"

### ğŸ”´ Critical Gaps (Grade: D / 60%)

1. **No Centralized Error Dashboard**
   - Can't see "top error categories this week"
   - Can't compare error rates across scrapers
   - No trending or time-based analysis

2. **Complex Scraper Chains Invisible**
   - Cinema City: CinemaDateJob â†’ MovieDetailJob â†’ ShowtimeProcessJob
   - Kino Krakow: SyncJob â†’ DayPageJob â†’ MovieDetailJob + ShowtimeProcessJob
   - Can't see cascade failures or bottlenecks

3. **No Error Pattern Analysis**
   - Can't answer "Why do 15% of jobs fail?"
   - Can't identify "80% of validation errors are from QuestionOne"
   - No correlation between error spikes and external factors

4. **Missing Success Targets**
   - No SLOs defined (e.g., "QuestionOne should be >95% successful")
   - No automated alerts on degradation
   - No baseline to measure improvements against

5. **Broken Job Monitor Route**
   - `/admin/jobs-monitor` returns 500 error
   - Indicates duplicate/conflicting monitoring systems

---

## Architecture Analysis

### Data Flow (Current)

```
1. Oban Worker perform/1
   â†“
2. MetricsTracker.record_success/failure (IF used)
   â†’ Updates Oban.Job.meta with error_category
   â†“
3. ObanTelemetry :stop event
   â†’ Creates JobExecutionSummary
   â†’ Merges job.meta into results field
   â†“
4. JobExecutionSummaries context
   â†’ Query functions for UI
   â†“
5. JobExecutionMonitorLive
   â†’ Displays data (poorly formatted)
```

### Key Insight

**The infrastructure exists! The problem is:**
1. MetricsTracker not used consistently
2. UI doesn't surface what's in the database
3. Error categories stored but not filterable/trendable

### Architecture Strengths

âœ… **Telemetry-based**: Automatic tracking without worker code changes
âœ… **Flexible schema**: JSONB allows scraper-specific metrics
âœ… **Historical**: Data retained beyond Oban's default
âœ… **Extensible**: Easy to add new error categories or metrics

### Architecture Weaknesses

âŒ **Opt-in MetricsTracker**: Not enforced, easy to forget
âŒ **Dual monitoring systems**: job-executions vs jobs-monitor confusion
âŒ **No typed results**: JSONB makes querying error categories harder
âŒ **No correlation tracking**: Can't link failed MovieDetailJob â†’ skipped ShowtimeProcessJobs

---

## Detailed Gap Analysis

### Gap 1: Error Category Filtering

**Current State**: Error categories exist in `results` JSONB but not queryable.

**Desired State**:
```sql
-- Should be able to query:
SELECT * FROM job_execution_summaries
WHERE results->>'error_category' = 'validation_error'
```

**UI Should Show**:
- Filter dropdown: "Show only: Validation Errors"
- Visual badges with colors (red=validation, yellow=network, etc.)
- Error category breakdown chart

### Gap 2: Silent Failure Visibility

**Current State**: `detect_silent_failures(hours_back)` exists but not called in UI.

**Desired State**:
- Prominent alert banner: "âš ï¸ 12 silent failures in last 24h"
- Dedicated section showing jobs that succeeded but created 0 entities
- Drill-down to investigate why (API returned empty? Parsing failed?)

**Example Silent Failure**:
```elixir
# Job completes successfully
{:ok, %{movies_scheduled: 0, showtimes_scheduled: 0}}

# But nothing was created - why?
# - Website structure changed?
# - API returned empty?
# - Date range issue?
```

### Gap 3: Scraper Chain Visualization

**Current State**: Individual job execution logs, no relationship visibility.

**Desired State** (for Cinema City example):
```
CinemaDateJob [ID: 123]
â”œâ”€ Status: âœ… Completed (1.2s)
â”œâ”€ Results: 5 cinemas, 15 dates processed
â”œâ”€ Spawned: 3 MovieDetailJobs, 45 ShowtimeProcessJobs
â”‚
â”œâ”€â†’ MovieDetailJob [ID: 456]
â”‚   â”œâ”€ Status: âœ… Matched TMDB (850ms)
â”‚   â””â”€ Results: tmdb_id=12345, confidence=0.95
â”‚
â”œâ”€â†’ MovieDetailJob [ID: 457]
â”‚   â”œâ”€ Status: âŒ Failed (350ms)
â”‚   â””â”€ Error: validation_error - Missing TMDB match
â”‚
â””â”€â†’ ShowtimeProcessJob [ID: 789]
    â”œâ”€ Status: â­ï¸ Skipped
    â””â”€ Reason: Depends on MovieDetailJob #457 (failed)
```

**Implementation**: Use `results.parent_job_id` (already captured by telemetry!)

### Gap 4: Error Pattern Trending

**Current State**: Can see current error counts, not historical trends.

**Desired State**:
```
QuestionOne - Last 7 Days
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Total Jobs:     1,234
Success Rate:   92.3% (â†“ 2.1% from last week)

Error Breakdown:
ğŸ“Š validation_error    45 (60%) â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘
ğŸ“Š network_error       20 (27%) â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘
ğŸ“Š geocoding_error     10 (13%) â–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘

Top Error: "Event title is required" (23 occurrences)
Recommendation: Add validation before scheduling jobs
```

### Gap 5: Success Rate Targets (SLOs)

**Desired State**: Define and track Service Level Objectives per scraper.

**Proposed SLOs**:
```elixir
# In scraper config
@target_success_rate 0.95  # 95% of jobs should succeed
@target_avg_duration 2000  # Avg duration < 2 seconds
@alert_threshold 0.85      # Alert if success rate < 85%

# UI should show:
QuestionOne: âœ… 96.2% (Target: 95%)
CinemaCity:  âš ï¸ 88.1% (Target: 95%, Alert: 85%)
KinoKrakow:  âŒ 78.3% (Below alert threshold!)
```

---

## Phased Implementation Plan

### Phase 1: Foundation (Week 1-2) - **High Priority**

**Goal**: Make existing error categories visible and filterable.

#### 1.1 UI Enhancements for Error Categories

**File**: `lib/eventasaurus_web/live/admin/job_execution_monitor_live.ex`

- [ ] Add error category filter dropdown
- [ ] Add color-coded error badges (validation=red, network=yellow, etc.)
- [ ] Show error category breakdown chart on dashboard
- [ ] Make "Results" column parse and format error_category/error_message cleanly

**Acceptance Criteria**:
- Can filter executions by error category
- Error categories visually distinct with colors
- Can see "15 validation errors, 8 network errors" at a glance

#### 1.2 Surface Silent Failures

**File**: `lib/eventasaurus_web/live/admin/job_execution_monitor_live.ex`

- [ ] Call `detect_silent_failures(24)` on mount
- [ ] Add alert banner if silent failures > threshold (e.g., 5)
- [ ] Add "Silent Failures" section to dashboard
- [ ] Show which scrapers have highest silent failure rates

**Acceptance Criteria**:
- Dashboard shows "âš ï¸ 12 silent failures in last 24h"
- Can click to see list of silent failures with details
- Can investigate individual silent failure jobs

#### 1.3 Fix Broken Jobs Monitor

**Files**:
- `lib/eventasaurus_web/live/admin/job_monitor_live.ex`
- `lib/eventasaurus_web/live/aggregated_content_live.ex` (compilation error)

- [ ] Fix compilation error in aggregated_content_live.ex
- [ ] Decide: consolidate with job-executions or remove?
- [ ] Update routes and navigation

**Acceptance Criteria**:
- No 500 errors on /admin/jobs-monitor
- Clear separation or consolidation of monitoring UIs

### Phase 2: Pilot Integration (Week 3-4) - **Medium Priority**

**Goal**: Integrate MetricsTracker across Cinema City and Kino Krakow as pilot scrapers.

#### 2.1 Cinema City Integration

**Files to Update**:
- `lib/eventasaurus_discovery/sources/cinema_city/jobs/cinema_date_job.ex`
- `lib/eventasaurus_discovery/sources/cinema_city/jobs/movie_detail_job.ex`
- `lib/eventasaurus_discovery/sources/cinema_city/jobs/showtime_process_job.ex` âœ… (Already uses MetricsTracker)

**Tasks**:
- [ ] Add MetricsTracker.record_success/failure to CinemaDateJob
- [ ] Add MetricsTracker.record_success/failure to MovieDetailJob
- [ ] Verify ShowtimeProcessJob usage (already implemented)
- [ ] Test end-to-end error categorization
- [ ] Document usage pattern for other scrapers

**Expected Error Categories**:
- CinemaDateJob: `network_error` (API failures), `data_quality_error` (parsing issues)
- MovieDetailJob: `validation_error` (missing TMDB match), `network_error` (TMDB API down)
- ShowtimeProcessJob: `validation_error` (missing required fields), `duplicate_error`

#### 2.2 Kino Krakow Integration

**Files to Update**:
- `lib/eventasaurus_discovery/sources/kino_krakow/jobs/sync_job.ex`
- `lib/eventasaurus_discovery/sources/kino_krakow/jobs/day_page_job.ex`
- `lib/eventasaurus_discovery/sources/kino_krakow/jobs/movie_detail_job.ex`
- `lib/eventasaurus_discovery/sources/kino_krakow/jobs/showtime_process_job.ex`

**Tasks**:
- [ ] Add MetricsTracker to all 4 jobs
- [ ] Test complex chain: SyncJob â†’ DayPageJob â†’ (MovieDetailJob + ShowtimeProcessJob)
- [ ] Verify error categorization across chain
- [ ] Document common error patterns

**Expected Error Categories**:
- SyncJob: `network_error` (site down), `data_quality_error` (CSRF token issues)
- DayPageJob: `network_error`, `data_quality_error` (HTML parsing)
- MovieDetailJob: `validation_error` (TMDB matching), `network_error`
- ShowtimeProcessJob: Same as Cinema City

#### 2.3 Scraper Chain Visualization

**New Feature**: Job lineage tree view

**Files**:
- `lib/eventasaurus_discovery/job_execution_summaries/lineage.ex` (exists!)
- `lib/eventasaurus_web/live/admin/job_execution_monitor_live.ex` (add tree view component)

**Tasks**:
- [ ] Enhance existing Lineage module for parent-child queries
- [ ] Add "View Chain" button next to "View Tree" for coordinator jobs
- [ ] Build visual tree component showing job hierarchy
- [ ] Color-code jobs by state (green=success, red=failed, gray=skipped)
- [ ] Show cascade failures (parent failed â†’ children skipped)

**Example UI**:
```
Cinema City Sync [2025-01-22 10:00]
â”œâ”€ CinemaDateJob: âœ… 5 cinemas processed
â”‚  â”œâ”€ MovieDetailJob (Film A): âœ… Matched TMDB
â”‚  â”‚  â””â”€ ShowtimeProcessJob: âœ… 12 events created
â”‚  â”œâ”€ MovieDetailJob (Film B): âŒ Validation error
â”‚  â”‚  â””â”€ ShowtimeProcessJob: â­ï¸ Skipped (dependency failed)
â”‚  â””â”€ MovieDetailJob (Film C): âœ… Matched TMDB
â”‚     â””â”€ ShowtimeProcessJob: âœ… 8 events created
â””â”€ Summary: 20/25 jobs succeeded (80%)
```

### Phase 3: Trending & Analysis (Week 5-6) - **Lower Priority**

**Goal**: Add historical trending and error pattern analysis.

#### 3.1 Error Trending Dashboard

**New LiveView**: `lib/eventasaurus_web/live/admin/error_trends_live.ex`

**Features**:
- [ ] Line chart: Error rate over time (7d, 30d, 90d)
- [ ] Breakdown by error category over time
- [ ] Top 10 most common error messages
- [ ] Scraper comparison: Success rates side-by-side
- [ ] Export to CSV for further analysis

**Queries to Add**:
```elixir
# In JobExecutionSummaries context
def get_error_trends(hours_back, granularity: :hour | :day)
def get_top_error_messages(hours_back, limit: 20)
def compare_scrapers(hours_back)
```

#### 3.2 Success Rate Targets (SLOs)

**New Module**: `lib/eventasaurus_discovery/metrics/scraper_slos.ex`

**Features**:
- [ ] Define target success rates per scraper
- [ ] Define target avg duration per scraper
- [ ] Define alert thresholds
- [ ] Dashboard shows actual vs target
- [ ] Visual indicators (âœ… âš ï¸ âŒ) based on thresholds

**Configuration Example**:
```elixir
defmodule EventasaurusDiscovery.Metrics.ScraperSLOs do
  @slos %{
    "question_one" => %{
      target_success_rate: 0.95,
      target_avg_duration_ms: 2000,
      alert_threshold: 0.85
    },
    "cinema_city" => %{
      target_success_rate: 0.90,  # Lower due to TMDB matching complexity
      target_avg_duration_ms: 3000,
      alert_threshold: 0.80
    }
  }
end
```

### Phase 4: Rollout to All Scrapers (Week 7-8) - **Future**

**Goal**: Standardize MetricsTracker usage across all scrapers.

#### 4.1 Documentation & Guidelines

- [ ] Write "Scraper Monitoring Guide" in docs/
- [ ] Add MetricsTracker to scraper template/generator
- [ ] Create checklist for new scraper development
- [ ] Add tests verifying MetricsTracker usage

#### 4.2 Remaining Scrapers

Apply MetricsTracker pattern to:
- [ ] QuestionOne (3 jobs)
- [ ] WeekPl (3 jobs)
- [ ] Ticketmaster (2 jobs)
- [ ] Pubquiz (2 jobs)
- [ ] All others (~20 more jobs)

**Batch Strategy**:
1. Start with highest-volume scrapers (QuestionOne, WeekPl)
2. Then highest-error-rate scrapers
3. Then all remaining

---

## Cinema City & Kino Krakow Specific Recommendations

### Cinema City Job Chain Analysis

**Current Chain**:
```
CinemaDateJob (coordinator)
  â†“
  â”œâ”€ Fetches cinema+date combinations
  â”œâ”€ Schedules MovieDetailJobs (per unique film)
  â””â”€ Schedules ShowtimeProcessJobs (per showtime)
```

**Monitoring Needs**:
1. **CinemaDateJob Failures**
   - Error Category: `network_error` (Cinema City API down)
   - Error Category: `data_quality_error` (Unexpected API response)
   - **Recommendation**: Add retry logic with exponential backoff

2. **MovieDetailJob Failures**
   - Error Category: `validation_error` (No TMDB match found)
   - **Silent Failure**: Job succeeds but match confidence < 0.7
   - **Recommendation**: Track match confidence in results, alert on low confidence

3. **ShowtimeProcessJob Failures**
   - Error Category: `validation_error` (Missing required event fields)
   - Error Category: `duplicate_error` (Event already exists)
   - **Recommendation**: Most errors expected, track proportion

**Success Metrics**:
- **CinemaDateJob**: 95%+ success (should almost never fail)
- **MovieDetailJob**: 85%+ success (TMDB matching is hard)
- **ShowtimeProcessJob**: 90%+ success (duplicates are OK)

**Specific Improvements**:
```elixir
# In CinemaDateJob perform/1
result =
  case fetch_cinema_data() do
    {:ok, data} ->
      MetricsTracker.record_success(job, "cinema_city_sync_#{date}")
      {:ok, %{cinemas_processed: length(data)}}

    {:error, %HTTPoison.Error{reason: :timeout}} ->
      MetricsTracker.record_failure(job, "Network timeout", "cinema_city_sync_#{date}")
      {:error, :network_timeout}

    {:error, reason} ->
      MetricsTracker.record_failure(job, reason, "cinema_city_sync_#{date}")
      {:error, reason}
  end
```

### Kino Krakow Job Chain Analysis

**Current Chain**:
```
SyncJob (coordinator)
  â†“
  â”œâ”€ Initializes session, gets CSRF token
  â””â”€ Schedules DayPageJobs (per day, 0-6)
       â†“
       â”œâ”€ Extracts showtimes for day
       â”œâ”€ Schedules MovieDetailJobs (per unique movie)
       â””â”€ Schedules ShowtimeProcessJobs (per showtime)
```

**Monitoring Needs**:
1. **SyncJob Failures** (Critical - blocks entire scrape)
   - Error Category: `network_error` (Site down, timeout)
   - Error Category: `data_quality_error` (CSRF token extraction failed)
   - **Recommendation**: Add health check before scheduling 7 DayPageJobs

2. **DayPageJob Failures**
   - Error Category: `network_error` (Day-specific page timeout)
   - Error Category: `data_quality_error` (HTML structure changed)
   - **Impact**: Loss of 1/7th of data (one day)
   - **Recommendation**: Retry with backoff, alert if >2 days fail

3. **MovieDetailJob Failures**
   - Same as Cinema City (TMDB matching)
   - **Additional Issue**: Kino Krakow uses different title format
   - **Recommendation**: Track match method (exact, fuzzy, year-based)

4. **ShowtimeProcessJob Failures**
   - Same as Cinema City

**Success Metrics**:
- **SyncJob**: 95%+ success (critical coordinator)
- **DayPageJob**: 90%+ success (7 jobs per sync, expect 1-2 failures)
- **MovieDetailJob**: 80%+ success (harder than Cinema City due to titles)
- **ShowtimeProcessJob**: 90%+ success

**Specific Improvements**:
```elixir
# In SyncJob perform/1
with {:ok, cookies} <- init_session(),
     {:ok, csrf_token} <- extract_csrf_token(cookies),
     {:ok, scheduled} <- schedule_day_jobs(csrf_token) do

  MetricsTracker.record_success(job, "kino_krakow_sync_#{Date.utc_today()}")
  {:ok, %{days_scheduled: scheduled}}
else
  {:error, :csrf_token_not_found} = error ->
    MetricsTracker.record_failure(job, "CSRF token extraction failed", external_id)
    error

  {:error, reason} = error ->
    MetricsTracker.record_failure(job, reason, external_id)
    error
end
```

**Silent Failure Detection for Movies**:
```elixir
# In DayPageJob perform/1
result = {:ok, %{
  showtimes_count: length(showtimes),
  unique_movies: length(unique_movies),
  movies_scheduled: movies_scheduled,
  showtimes_scheduled: showtimes_scheduled
}}

# Silent failure if:
# - showtimes_count = 0 (likely HTML parsing issue)
# - movies_scheduled = 0 but showtimes_count > 0 (movie extraction failed)
```

---

## Success Metrics & Validation

### Phase 1 Success Criteria (Foundation)

**Quantitative**:
- [ ] Can filter job executions by error category (9 categories)
- [ ] Error category breakdown chart shows on dashboard
- [ ] Silent failures detected and displayed (count + list)
- [ ] Zero 500 errors on monitoring pages
- [ ] <2 second page load time for job executions dashboard

**Qualitative**:
- [ ] Can answer "What types of errors happened today?" in <10 seconds
- [ ] Can identify silent failures without checking logs
- [ ] Error information readable without parsing text strings

### Phase 2 Success Criteria (Pilot Integration)

**Quantitative**:
- [ ] 100% of Cinema City jobs use MetricsTracker (3/3 jobs)
- [ ] 100% of Kino Krakow jobs use MetricsTracker (4/4 jobs)
- [ ] Can view job chain for coordinator jobs
- [ ] Error categorization working for 7 total jobs

**Qualitative**:
- [ ] Can answer "Why did 20% of Cinema City jobs fail?" in <30 seconds
- [ ] Can see cascade failures (parent failed â†’ children skipped)
- [ ] Can identify if TMDB matching is the bottleneck

### Phase 3 Success Criteria (Trending & Analysis)

**Quantitative**:
- [ ] Can view error trends over 7d, 30d, 90d
- [ ] Can compare success rates across scrapers
- [ ] SLOs defined for 5+ scrapers
- [ ] Dashboard shows actual vs target success rates

**Qualitative**:
- [ ] Can answer "Is QuestionOne getting worse?" in <30 seconds
- [ ] Can identify "validation errors spiked on Jan 15th"
- [ ] Can prioritize improvements based on data

### Phase 4 Success Criteria (Full Rollout)

**Quantitative**:
- [ ] 100% of scraper jobs use MetricsTracker (50+ jobs)
- [ ] <5% console log usage for debugging
- [ ] Documentation exists for monitoring usage
- [ ] Tests enforce MetricsTracker usage

**Qualitative**:
- [ ] New scraper developers use monitoring without prompting
- [ ] Can investigate any scraper failure in <5 minutes
- [ ] Team makes data-driven scraper improvement decisions

---

## Implementation Notes

### Database Considerations

**No migrations needed!** All data already captured:
- `job_execution_summaries.results` stores error_category
- `job_execution_summaries.error` stores error message
- Lineage tracking via `results.parent_job_id`

**Potential Index**:
```sql
-- Speed up error category queries
CREATE INDEX idx_job_exec_error_category
ON job_execution_summaries ((results->>'error_category'));

-- Speed up silent failure detection
CREATE INDEX idx_job_exec_state_worker
ON job_execution_summaries (state, worker);
```

### Performance Considerations

1. **Dashboard Load Time**:
   - Current: Query last 50 executions (fast)
   - Phase 3: Query 90d of data (may be slow)
   - **Solution**: Materialize daily aggregates, query pre-computed

2. **Telemetry Overhead**:
   - ObanTelemetry already runs asynchronously (`Task.start`)
   - MetricsTracker updates happen in-process
   - **Impact**: Minimal (<5ms per job)

3. **JobExecutionSummary Growth**:
   - Current: ~300 jobs/day = ~110K rows/year
   - **Solution**: Already has cleanup function (`delete_old_summaries/1`)
   - **Recommendation**: Keep 90 days, delete older

### Testing Strategy

1. **Phase 1**: Manual testing on staging
2. **Phase 2**: Automated tests for MetricsTracker integration
3. **Phase 3**: Load testing for dashboard with 90d data
4. **Phase 4**: Integration tests enforcing MetricsTracker usage

---

## Current System Grade Breakdown

| Component | Grade | Score | Reasoning |
|-----------|-------|-------|-----------|
| **Infrastructure** | A | 95 | Excellent schema, telemetry, and context layer |
| **Error Categorization** | B+ | 87 | Module exists and works, but not consistently used |
| **UI/UX** | C+ | 78 | Functional but error details poorly formatted |
| **Adoption** | D | 65 | Only 2/20 scrapers use MetricsTracker |
| **Documentation** | D- | 60 | Limited guidance on using monitoring tools |
| **Trending/Analysis** | F | 50 | No historical trending or pattern analysis |
| **Silent Failures** | C | 75 | Detection exists but not surfaced in UI |
| **Complex Chains** | C- | 72 | Job lineage exists but visualization poor |
| **OVERALL** | **B-** | **75** | Solid foundation, poor adoption & UX |

---

## Recommendations

### Immediate Actions (Next Sprint)

1. âœ… **Fix broken jobs-monitor route** - Remove or consolidate
2. âœ… **Add error category filter** - Make existing data usable
3. âœ… **Surface silent failures** - Add dashboard alert banner

### Short-term (Next 4 weeks)

4. âœ… **Integrate Cinema City** - Pilot MetricsTracker across all 3 jobs
5. âœ… **Integrate Kino Krakow** - Pilot complex chain monitoring
6. âœ… **Add job chain visualization** - Leverage existing lineage tracking

### Medium-term (Next 2-3 months)

7. âœ… **Build error trending dashboard** - Historical analysis
8. âœ… **Define SLOs per scraper** - Success rate targets
9. âœ… **Rollout to all scrapers** - Standardize monitoring usage

### Long-term (Next 6 months)

10. âœ… **Automated alerting** - Slack/email on SLO violations
11. âœ… **Correlation analysis** - Link external events to failure spikes
12. âœ… **Predictive analytics** - Identify failure patterns before they happen

---

## Conclusion

**We have a B- monitoring system that could be an A with focused effort.**

The infrastructure is excellent. The problem is:
1. **Adoption**: MetricsTracker used in only 10% of scrapers
2. **UX**: Error data exists but poorly displayed
3. **Analysis**: No trending, comparison, or pattern detection

By focusing on Phase 1 (UI improvements) and Phase 2 (pilot integration), we can:
- âœ… Reduce debugging time from hours to minutes
- âœ… Identify error patterns we're currently blind to
- âœ… Make data-driven decisions about scraper improvements
- âœ… Increase overall success rates by 5-10%

**Next Steps**:
1. Review and approve this plan
2. Create tasks for Phase 1 (Week 1-2)
3. Assign Cinema City pilot integration
4. Schedule demo after Phase 1 completion

---

**Created**: 2025-01-22
**Author**: Claude Code Analysis
**Related Files**:
- `lib/eventasaurus_app/monitoring/oban_telemetry.ex`
- `lib/eventasaurus_discovery/job_execution_summaries.ex`
- `lib/eventasaurus_discovery/metrics/error_categories.ex`
- `lib/eventasaurus_discovery/metrics/metrics_tracker.ex`
- `lib/eventasaurus_web/live/admin/job_execution_monitor_live.ex`
