# Source Implementation Guide

**Version:** 1.0
**Status:** Proposed Standard
**Last Updated:** 2025-01-23

## Purpose

This guide defines the standard naming conventions, architecture patterns, and implementation requirements for all EventasaurusDiscovery data sources. Following these standards ensures:

- **Predictability:** Developers can navigate any source with familiarity
- **Maintainability:** Consistent patterns reduce cognitive load
- **Tooling:** AI agents and generators can work with predictable structures
- **Monitoring:** Standardized external IDs enable robust monitoring dashboards
- **Testing:** Shared test utilities work across all sources

## Table of Contents

1. [Quick Start: Using the Source Generator](#1-quick-start-using-the-source-generator)
2. [Job Naming Conventions](#2-job-naming-conventions)
3. [External ID Format Specification](#3-external-id-format-specification)
4. [Module Structure Template](#4-module-structure-template)
5. [BaseJob Adoption Pattern](#5-basejob-adoption-pattern)
6. [MetricsTracker Integration](#6-metricstracker-integration)
7. [Testing Requirements](#7-testing-requirements)
8. [Examples](#8-examples)
9. [Migration Guide](#9-migration-guide)
10. [Checklist for New Sources](#10-checklist-for-new-sources)
11. [Architecture Decision Records (ADRs)](#11-architecture-decision-records-adrs)
12. [Zyte API Usage Guidelines](#12-zyte-api-usage-guidelines-critical)
13. [Job Args Standards](#13-job-args-standards-critical)
14. [Deduplication Strategies](#14-deduplication-strategies)

---

## 1. Quick Start: Using the Source Generator

### Overview

The fastest way to create a new source is using the built-in generator:

```bash
mix discovery.generate_source my_source
```

This command generates a complete source structure following all standards in this guide, including:
- âœ… Standard directory structure
- âœ… Stub modules with TODO comments and examples
- âœ… MetricsTracker integration in all jobs
- âœ… External IDs following the standard format
- âœ… Test files with example test cases
- âœ… BaseJob integration (optional)

### Usage

```bash
# Generate basic source with BaseJob (recommended for most sources)
mix discovery.generate_source my_source

# Generate source with index and detail jobs
mix discovery.generate_source my_source --with-index --with-detail

# Generate source without BaseJob (for custom orchestration)
mix discovery.generate_source my_source --no-base-job

# Force overwrite existing files
mix discovery.generate_source my_source --force
```

### Options

| Flag | Description | Default |
|------|-------------|---------|
| `--base-job` | Use BaseJob behavior for SyncJob | `true` |
| `--no-base-job` | Custom implementation without BaseJob | - |
| `--with-index` | Generate IndexPageJob for pagination | `false` |
| `--with-detail` | Generate EventDetailJob for detail fetching | `false` |
| `--force` | Overwrite existing files | `false` |

### Generated Structure

```
lib/eventasaurus_discovery/sources/my_source/
â”œâ”€â”€ client.ex                 # HTTP client wrapper with examples
â”œâ”€â”€ config.ex                 # Configuration constants
â”œâ”€â”€ transformer.ex            # Raw data â†’ unified format
â””â”€â”€ jobs/
    â”œâ”€â”€ sync_job.ex           # Main orchestration job (BaseJob or custom)
    â”œâ”€â”€ index_page_job.ex     # Optional: pagination (--with-index)
    â””â”€â”€ event_detail_job.ex   # Optional: detail fetching (--with-detail)

test/eventasaurus_discovery/sources/my_source/
â”œâ”€â”€ client_test.exs           # Client tests with examples
â”œâ”€â”€ transformer_test.exs      # Transformer tests
â””â”€â”€ jobs/
    â””â”€â”€ sync_job_test.exs     # SyncJob tests
```

### Post-Generation Steps

After running the generator, complete these steps:

1. **Update Configuration** (`config.ex`)
   - Set API base URL and authentication details
   - Configure rate limits based on API documentation
   - Set retry settings

2. **Implement HTTP Client** (`client.ex`)
   - Complete the `fetch_events/3` function
   - Add authentication headers
   - Handle pagination if needed
   - Implement error handling

3. **Implement Transformer** (`transformer.ex`)
   - Map raw API data to standardized event format
   - Extract all required fields (title, dates, venue, etc.)
   - Build proper external IDs
   - Add validation and error handling

4. **Complete Job Implementation** (`jobs/sync_job.ex`)
   - **If using BaseJob:** Verify `fetch_events/3` and `transform_events/1` callbacks
   - **If custom:** Implement full sync workflow
   - Test MetricsTracker integration
   - Add any child job enqueueing logic

5. **Write Tests**
   - Add HTTP mocks for client tests
   - Test transformation edge cases
   - Verify job execution and error handling
   - Test MetricsTracker integration

6. **Register Source**
   - Add source to `lib/mix/tasks/discovery.sync.ex`:
     ```elixir
     @sources %{
       # ... existing sources
       "my_source" => EventasaurusDiscovery.Sources.MySource.Jobs.SyncJob
     }
     ```

7. **Test Your Source**
   ```bash
   # Run tests
   mix test test/eventasaurus_discovery/sources/my_source/

   # Test sync with small limit
   mix discovery.sync my_source --limit 10 --inline
   ```

### Generated Code Examples

**Generated SyncJob with BaseJob:**
```elixir
defmodule EventasaurusDiscovery.Sources.MySource.Jobs.SyncJob do
  use EventasaurusDiscovery.Sources.BaseJob

  @impl true
  def fetch_events(from_date, to_date, context) do
    Client.fetch_events(from_date, to_date, context)
  end

  @impl true
  def transform_events(raw_events) do
    Transformer.transform_events(raw_events)
  end

  @impl true
  def source_config do
    %{
      source_slug: Config.source_slug(),
      rate_limit: Config.rate_limit(),
      retry_config: Config.retry_config()
    }
  end
end
```

**Generated Transformer:**
```elixir
defmodule EventasaurusDiscovery.Sources.MySource.Transformer do
  def transform_event(raw_event) do
    %{
      external_id: build_external_id(raw_event),
      title: extract_title(raw_event),
      description: extract_description(raw_event),
      start_time: extract_start_time(raw_event),
      end_time: extract_end_time(raw_event),
      venue: extract_venue(raw_event),
      performers: extract_performers(raw_event),
      categories: extract_categories(raw_event),
      source_url: extract_source_url(raw_event),
      source_data: raw_event
    }
  end

  defp build_external_id(raw_event) do
    # Format: {source}_{type}_{id}_{date}
    source_id = raw_event["id"]
    date = Date.utc_today() |> Date.to_string()
    "my_source_event_#{source_id}_#{date}"
  end
end
```

All generated code includes TODO comments marking areas that need implementation.

---

## 2. Job Naming Conventions

### 8.1 Naming Pattern

**Rule:** All job modules MUST follow the pattern:

```
EventasaurusDiscovery.Sources.{SourceSlug}.Jobs.{JobType}Job
```

**File naming:** Convert to snake_case for file names:

```
lib/eventasaurus_discovery/sources/{source_slug}/jobs/{job_type}_job.ex
```

### 8.2 Standard Job Types

Job types MUST be one of the following (in typical execution order):

| Job Type | Purpose | Required | File Name |
|----------|---------|----------|-----------|
| **SyncJob** | Coordinator/orchestrator for entire source | âœ… YES | `sync_job.ex` |
| **IndexPageJob** | Fetches index/listing pages (with pagination) | Optional | `index_page_job.ex` |
| **IndexJob** | Fetches single index (no pagination) | Optional | `index_job.ex` |
| **EventDetailJob** | Event-specific detail fetcher | Optional | `event_detail_job.ex` |
| **VenueDetailJob** | Venue-specific detail fetcher | Optional | `venue_detail_job.ex` |
| **PerformerDetailJob** | Performer/artist-specific detail fetcher | Optional | `performer_detail_job.ex` |
| **EnrichmentJob** | Post-processing enrichment (metadata, images, etc.) | Optional | `enrichment_job.ex` |

### 8.3 Job Naming Patterns by Source Type

Different source types use context-appropriate job names based on their domain:

| Source Type | Job Pattern | Example | Rationale |
|-------------|-------------|---------|-----------|
| **Event Sources** | `EventDetailJob` | ResidentAdvisor, Bandsintown | General events (concerts, festivals) |
| **Cinema Sources** | `MovieDetailJob` | CinemaCity, KinoKrakow | Domain-specific terminology for movie screenings |
| **Venue Sources** | `VenueDetailJob` | Karnet, QuestionOne | Venue-focused discovery |
| **Performer Sources** | `PerformerEnrichmentJob` | ResidentAdvisor | Post-processing enrichment (not detail fetching) |

**Legacy Migration:**

| âŒ Legacy Name | âœ… Standard Name | Reason |
|---------------|-----------------|---------|
| `index_job.ex` | `index_page_job.ex` | Clarifies pagination support |

**Note:** Sources may use domain-specific terminology (e.g., `MovieDetailJob` for cinema sources) when it provides clearer abstraction. This is preferred over forcing generic names that don't match the source's domain model.

### 8.4 Examples

âœ… **Correct:**
```
EventasaurusDiscovery.Sources.CinemaCity.Jobs.SyncJob
EventasaurusDiscovery.Sources.Karnet.Jobs.VenueDetailJob
EventasaurusDiscovery.Sources.ResidentAdvisor.Jobs.PerformerDetailJob
```

âŒ **Incorrect:**
```
EventasaurusDiscovery.Sources.CinemaCity.Jobs.cinema_city_sync  (lowercase, wrong pattern)
EventasaurusDiscovery.Sources.Karnet.Jobs.VenueJob              (missing "Detail")
EventasaurusDiscovery.Sources.ResidentAdvisor.Jobs.ArtistDetailJob  (should be PerformerDetailJob)
```

---

## 3. External ID Format Specification

### 8.1 Format

External IDs MUST follow the pattern:

```
{source_slug}_{job_type}_{entity_id}_{date}
```

**Components:**
- `source_slug`: Source identifier (snake_case, e.g., `cinema_city`, `week_pl`)
- `job_type`: Job classification (snake_case, e.g., `sync`, `venue`, `event`, `performer`)
- `entity_id`: Entity identifier (optional, use for detail jobs)
- `date`: ISO 8601 date with **hyphens** (e.g., `2025-01-23`)

### 8.2 Rules

1. **Date Format:** MUST use hyphens (`2025-01-23`), NOT underscores (`2025_01_23`)
2. **Entity ID:** Optional for coordinator jobs (SyncJob), REQUIRED for detail jobs
3. **Separator:** Components MUST be separated by single underscore `_`
4. **Uniqueness:** External ID should be unique per job execution within a reasonable time window
5. **No URLs:** Do NOT use full URLs as external IDs
6. **No Special Characters:** Avoid spaces, slashes, or other special characters

### 8.3 Examples by Job Type

**SyncJob (no entity ID):**
```
cinema_city_sync_2025-01-23
karnet_sync_2025-01-23
week_pl_sync_2025-01-23
```

**VenueDetailJob (with entity ID):**
```
cinema_city_venue_12345_2025-01-23
karnet_venue_warszawa-centralna_2025-01-23
```

**EventDetailJob (with entity ID):**
```
resident_advisor_event_abc123_2025-01-23
sortiraparis_article_98765_2025-01-23
```

**PerformerDetailJob (with entity ID):**
```
resident_advisor_performer_dj-smith_2025-01-23
bandsintown_performer_789456_2025-01-23
```

### 8.4 Benefits

- âœ… **Parseable:** Easy to extract source, job type, entity, and date
- âœ… **Sortable:** Chronological sorting works naturally
- âœ… **Filterable:** Monitor dashboards can filter by any component
- âœ… **Readable:** Human-friendly format for debugging
- âœ… **Searchable:** Easy to find specific job executions

---

## 4. Module Structure Template

### 8.1 Standard Directory Layout

Every source MUST follow this structure:

```
lib/eventasaurus_discovery/sources/{source_slug}/
â”œâ”€â”€ client.ex                    # HTTP client wrapper (REQUIRED)
â”œâ”€â”€ config.ex                    # Configuration constants (REQUIRED)
â”œâ”€â”€ transformer.ex               # Raw â†’ unified format (REQUIRED)
â”œâ”€â”€ source.ex                    # Source metadata/config (OPTIONAL)
â”œâ”€â”€ extractors/                  # HTML/JSON parsing (if needed)
â”‚   â”œâ”€â”€ index_extractor.ex
â”‚   â”œâ”€â”€ detail_extractor.ex
â”‚   â””â”€â”€ showtime_extractor.ex
â”œâ”€â”€ jobs/                        # Oban workers (REQUIRED)
â”‚   â”œâ”€â”€ sync_job.ex              # REQUIRED
â”‚   â”œâ”€â”€ index_page_job.ex
â”‚   â”œâ”€â”€ event_detail_job.ex
â”‚   â””â”€â”€ venue_detail_job.ex
â””â”€â”€ helpers/                     # Source-specific utilities (if needed)
    â”œâ”€â”€ url_filter.ex
    â””â”€â”€ date_parser.ex
```

### 8.2 Core Modules

#### Client Module

**Purpose:** HTTP client wrapper with rate limiting and error handling

**Required Functions:**
```elixir
@spec fetch_index(map()) :: {:ok, binary()} | {:error, term()}
@spec fetch_detail(String.t(), map()) :: {:ok, binary()} | {:error, term()}
```

**Pattern:**
```elixir
defmodule EventasaurusDiscovery.Sources.{SourceSlug}.Client do
  @moduledoc """
  HTTP client for {Source Name} API/website.

  Handles rate limiting, authentication, and error responses.
  """

  use Tesla
  require Logger

  plug Tesla.Middleware.BaseUrl, "https://api.source.com"
  plug Tesla.Middleware.Headers, [{"user-agent", "Eventasaurus/1.0"}]
  plug Tesla.Middleware.JSON
  plug Tesla.Middleware.Retry, max_retries: 3, delay: 1_000

  @rate_limit_ms 1_000  # 1 request per second

  def fetch_index(opts \\ %{}) do
    :timer.sleep(@rate_limit_ms)

    case get("/events", query: opts) do
      {:ok, %Tesla.Env{status: 200, body: body}} ->
        {:ok, body}

      {:ok, %Tesla.Env{status: status}} ->
        {:error, "HTTP #{status}"}

      {:error, reason} ->
        {:error, reason}
    end
  end
end
```

#### Config Module

**Purpose:** Configuration constants and source metadata

**Required Constants:**
```elixir
@source_slug :: String.t()
@source_name :: String.t()
@base_url :: String.t()
@rate_limit_ms :: integer()
```

**Pattern:**
```elixir
defmodule EventasaurusDiscovery.Sources.{SourceSlug}.Config do
  @moduledoc """
  Configuration constants for {Source Name}.
  """

  @source_slug "{source_slug}"
  @source_name "{Source Display Name}"
  @base_url "https://source.com"
  @rate_limit_ms 1_000

  def source_slug, do: @source_slug
  def source_name, do: @source_name
  def base_url, do: @base_url
  def rate_limit_ms, do: @rate_limit_ms

  # Source-specific constants
  @default_limit 100
  @max_retries 3

  def default_limit, do: @default_limit
  def max_retries, do: @max_retries
end
```

#### Transformer Module

**Purpose:** Transform raw source data into standardized event format

**Required Function:**
```elixir
@spec transform(map()) :: EventasaurusDiscovery.Event.t()
```

**Pattern:**
```elixir
defmodule EventasaurusDiscovery.Sources.{SourceSlug}.Transformer do
  @moduledoc """
  Transforms {Source Name} raw data to Eventasaurus event format.
  """

  alias EventasaurusDiscovery.Event

  def transform(raw_event) do
    %Event{
      title: extract_title(raw_event),
      description: extract_description(raw_event),
      starts_at: parse_datetime(raw_event["start_time"]),
      ends_at: parse_datetime(raw_event["end_time"]),
      venue_name: raw_event["venue"],
      external_id: build_external_id(raw_event),
      source_url: raw_event["url"],
      metadata: %{
        "source" => Config.source_slug(),
        "raw_id" => raw_event["id"]
      }
    }
  end

  defp build_external_id(event) do
    "#{Config.source_slug()}_event_#{event["id"]}_#{Date.utc_today()}"
  end

  # ... helper functions
end
```

---

## 5. BaseJob Adoption Pattern

### 8.1 When to Use BaseJob

**Use BaseJob when:**
- âœ… Job is a **SyncJob** coordinator
- âœ… Job follows simple fetch â†’ transform â†’ process pattern
- âœ… Job doesn't require complex orchestration logic
- âœ… Job processes city-specific data

**DO NOT use BaseJob when:**
- âŒ Job requires complex orchestration (use plain `Oban.Worker`)
- âŒ Job needs custom perform/1 logic with branching
- âŒ Job is a detail job (EventDetailJob, VenueDetailJob, etc.)

### 8.2 BaseJob Implementation

**Minimal Example:**
```elixir
defmodule EventasaurusDiscovery.Sources.ExampleSource.Jobs.SyncJob do
  @moduledoc """
  Coordinator job for ExampleSource integration.

  Fetches events for a city and processes them using BaseJob behavior.
  """

  use EventasaurusDiscovery.Sources.BaseJob,
    queue: :discovery,
    max_attempts: 3,
    priority: 1

  alias EventasaurusDiscovery.Sources.ExampleSource.{Client, Transformer, Config}

  # Required callback: Fetch raw events from source
  @impl EventasaurusDiscovery.Sources.BaseJob
  def fetch_events(city, limit, options) do
    case Client.fetch_events(city.name, limit: limit) do
      {:ok, events} ->
        {:ok, events}

      {:error, reason} ->
        {:error, reason}
    end
  end

  # Required callback: Transform raw events to standard format
  @impl EventasaurusDiscovery.Sources.BaseJob
  def transform_events(raw_events) do
    Enum.map(raw_events, &Transformer.transform/1)
  end

  # Required callback: Source configuration
  def source_config do
    %{
      name: Config.source_name(),
      slug: Config.source_slug(),
      website_url: Config.base_url()
    }
  end
end
```

### 8.3 Override perform/1 When Needed

**When to override:** Complex orchestration that doesn't fit BaseJob pattern

**Example:**
```elixir
defmodule EventasaurusDiscovery.Sources.ComplexSource.Jobs.SyncJob do
  use EventasaurusDiscovery.Sources.BaseJob,
    queue: :discovery,
    max_attempts: 3

  # Override perform for custom orchestration
  @impl Oban.Worker
  def perform(%Oban.Job{args: args} = job) do
    external_id = "complex_source_sync_#{Date.utc_today()}"

    # Custom logic here
    result = case check_preconditions() do
      {:ok, :can_proceed} ->
        # Use BaseJob pattern via super
        super(job)

      {:skip, reason} ->
        Logger.info("Skipping sync: #{reason}")
        {:ok, %{status: "skipped", reason: reason}}
    end

    # Track metrics
    case result do
      {:ok, _} -> MetricsTracker.record_success(job, external_id)
      {:error, reason} -> MetricsTracker.record_failure(job, reason, external_id)
    end

    result
  end

  # Still implement required callbacks for consistency
  @impl EventasaurusDiscovery.Sources.BaseJob
  def fetch_events(city, limit, options), do: # ...

  @impl EventasaurusDiscovery.Sources.BaseJob
  def transform_events(raw_events), do: # ...
end
```

---

## 6. MetricsTracker Integration

### 8.1 Required Integration

**Every job MUST integrate MetricsTracker for monitoring:**

```elixir
alias EventasaurusDiscovery.Metrics.MetricsTracker

def perform(%Oban.Job{} = job) do
  # Build external ID following standard format
  external_id = "{source}_{job_type}_{entity_id}_#{Date.utc_today()}"

  result = # ... job logic

  # Track metrics based on result
  case result do
    {:ok, data} ->
      MetricsTracker.record_success(job, external_id)
      result

    {:error, reason} ->
      MetricsTracker.record_failure(job, reason, external_id)
      result
  end
end
```

### 8.2 Error Categorization

**MetricsTracker.record_failure/3** supports error categorization:

**Standard Error Categories:**
```elixir
:network_error        # HTTP errors, timeouts, connection failures
:validation_error     # Data validation failures
:geocoding_error      # Geocoding service failures
:venue_error          # Venue matching/creation failures
:performer_error      # Performer matching/creation failures
:category_error       # Event categorization failures
:duplicate_error      # Duplicate detection issues
:data_quality_error   # Missing/invalid data from source
:unknown_error        # Uncategorized errors
```

**Usage Example:**
```elixir
case fetch_and_process() do
  {:ok, result} ->
    MetricsTracker.record_success(job, external_id)
    {:ok, result}

  {:error, %HTTPError{} = reason} ->
    MetricsTracker.record_failure(job, reason, external_id, :network_error)
    {:error, reason}

  {:error, %ValidationError{} = reason} ->
    MetricsTracker.record_failure(job, reason, external_id, :validation_error)
    {:error, reason}

  {:error, reason} ->
    MetricsTracker.record_failure(job, reason, external_id, :unknown_error)
    {:error, reason}
end
```

---

## 7. Testing Requirements

### 8.1 Required Tests

Every source MUST include:

1. **Client Tests**
   - HTTP request formatting
   - Rate limiting behavior
   - Error handling (404, 500, timeout)
   - Response parsing

2. **Transformer Tests**
   - Valid input â†’ correct output mapping
   - Missing field handling
   - Date/time parsing edge cases
   - External ID format validation

3. **Job Integration Tests**
   - Successful execution flow
   - Error handling and retry behavior
   - MetricsTracker integration
   - External ID format compliance

### 8.2 Test Structure

```
test/eventasaurus_discovery/sources/{source_slug}/
â”œâ”€â”€ client_test.exs
â”œâ”€â”€ transformer_test.exs
â””â”€â”€ jobs/
    â”œâ”€â”€ sync_job_test.exs
    â””â”€â”€ event_detail_job_test.exs
```

### 8.3 External ID Test Example

```elixir
defmodule EventasaurusDiscovery.Sources.ExampleSource.Jobs.SyncJobTest do
  use EventasaurusApp.DataCase, async: true

  describe "external ID format" do
    test "follows standard format for sync job" do
      job = %Oban.Job{args: %{}}
      external_id = SyncJob.build_external_id(job)

      # Format: {source}_{type}_{date}
      assert external_id =~ ~r/^example_source_sync_\d{4}-\d{2}-\d{2}$/
    end

    test "uses hyphens in date, not underscores" do
      job = %Oban.Job{args: %{}}
      external_id = SyncJob.build_external_id(job)

      # Should be 2025-01-23, not 2025_01_23
      refute external_id =~ ~r/\d{4}_\d{2}_\d{2}/
    end
  end
end
```

---

## 8. Examples

### 8.1 Complete Source Example (Simple)

**Directory structure:**
```
lib/eventasaurus_discovery/sources/simple_source/
â”œâ”€â”€ client.ex
â”œâ”€â”€ config.ex
â”œâ”€â”€ transformer.ex
â””â”€â”€ jobs/
    â””â”€â”€ sync_job.ex
```

**SyncJob (using BaseJob):**
```elixir
defmodule EventasaurusDiscovery.Sources.SimpleSource.Jobs.SyncJob do
  @moduledoc """
  Coordinator job for SimpleSource event synchronization.
  """

  use EventasaurusDiscovery.Sources.BaseJob,
    queue: :discovery,
    max_attempts: 3

  alias EventasaurusDiscovery.Sources.SimpleSource.{Client, Config, Transformer}

  @impl EventasaurusDiscovery.Sources.BaseJob
  def fetch_events(city, limit, _options) do
    Client.fetch_events(%{
      city: city.name,
      limit: limit
    })
  end

  @impl EventasaurusDiscovery.Sources.BaseJob
  def transform_events(raw_events) do
    Enum.map(raw_events, &Transformer.transform/1)
  end

  def source_config do
    %{
      name: Config.source_name(),
      slug: Config.source_slug(),
      website_url: Config.base_url()
    }
  end
end
```

### 8.2 Complete Source Example (Complex)

**Directory structure:**
```
lib/eventasaurus_discovery/sources/complex_source/
â”œâ”€â”€ client.ex
â”œâ”€â”€ config.ex
â”œâ”€â”€ transformer.ex
â”œâ”€â”€ extractors/
â”‚   â”œâ”€â”€ index_extractor.ex
â”‚   â””â”€â”€ detail_extractor.ex
â””â”€â”€ jobs/
    â”œâ”€â”€ sync_job.ex
    â”œâ”€â”€ index_page_job.ex
    â””â”€â”€ event_detail_job.ex
```

**SyncJob (custom orchestration):**
```elixir
defmodule EventasaurusDiscovery.Sources.ComplexSource.Jobs.SyncJob do
  @moduledoc """
  Coordinator job that orchestrates index and detail jobs.
  """

  use Oban.Worker,
    queue: :discovery,
    max_attempts: 3

  require Logger
  alias EventasaurusDiscovery.Sources.ComplexSource.Jobs.IndexPageJob
  alias EventasaurusDiscovery.Metrics.MetricsTracker

  @impl Oban.Worker
  def perform(%Oban.Job{args: args} = job) do
    external_id = "complex_source_sync_#{Date.utc_today()}"
    city_id = args["city_id"]

    Logger.info("Starting ComplexSource sync for city #{city_id}")

    result = schedule_index_jobs(city_id, job.id)

    case result do
      {:ok, jobs_count} ->
        Logger.info("Scheduled #{jobs_count} index jobs")
        MetricsTracker.record_success(job, external_id)
        {:ok, %{jobs_scheduled: jobs_count}}

      {:error, reason} ->
        Logger.error("Failed to schedule jobs: #{inspect(reason)}")
        MetricsTracker.record_failure(job, reason, external_id, :unknown_error)
        {:error, reason}
    end
  end

  defp schedule_index_jobs(city_id, parent_job_id) do
    # Schedule 7 days of index jobs
    jobs = for day_offset <- 0..6 do
      date = Date.add(Date.utc_today(), day_offset)

      %{
        "city_id" => city_id,
        "date" => Date.to_iso8601(date)
      }
      |> IndexPageJob.new(meta: %{parent_job_id: parent_job_id})
      |> Oban.insert()
    end

    case Enum.filter(jobs, fn {:error, _} -> true; _ -> false end) do
      [] -> {:ok, length(jobs)}
      errors -> {:error, "#{length(errors)} jobs failed to schedule"}
    end
  end
end
```

---

## 9. Migration Guide

### 8.1 Renaming Jobs

**Steps:**
1. Create new file with standard name
2. Copy implementation from old file
3. Update module name
4. Update all references in other modules
5. Run tests
6. Delete old file
7. Commit with clear message

**Example commit message:**
```
refactor(sources): rename Inquizition index_job â†’ index_page_job

Standardizes job naming to follow source implementation guide.

- Renamed: index_job.ex â†’ index_page_job.ex
- Updated module: Inquizition.Jobs.IndexJob â†’ Inquizition.Jobs.IndexPageJob
- Updated references in: sync_job.ex, tests
- All tests passing âœ…

Part of #2366 Phase 3
```

### 8.2 Standardizing External IDs

**Steps:**
1. Audit current external ID format in job file
2. Update external ID generation to follow standard format
3. Update tests to verify format
4. Run job in staging and verify monitoring dashboard
5. Deploy to production
6. Monitor for 24 hours

**Migration script** (if needed to update historical data):
```sql
-- Update job_execution_summaries with standardized external IDs
UPDATE job_execution_summaries
SET external_id = REPLACE(external_id, '_', '-')
WHERE worker LIKE 'EventasaurusDiscovery.Sources.%'
AND external_id ~ '\d{4}_\d{2}_\d{2}';  -- Find underscore dates
```

---

## 10. Checklist for New Sources

Use this checklist when implementing a new source:

- [ ] **Directory Structure**
  - [ ] Created `lib/eventasaurus_discovery/sources/{source_slug}/`
  - [ ] Created `client.ex`, `config.ex`, `transformer.ex`
  - [ ] Created `jobs/sync_job.ex`
  - [ ] Created additional job files as needed

- [ ] **Naming Conventions**
  - [ ] All job files follow `{job_type}_job.ex` pattern
  - [ ] Module names follow `{SourceSlug}.Jobs.{JobType}Job`
  - [ ] External IDs follow `{source}_{type}_{id}_{date}` format
  - [ ] Dates use hyphens, not underscores

- [ ] **BaseJob Integration**
  - [ ] SyncJob uses BaseJob (if applicable)
  - [ ] Implemented `fetch_events/3` callback
  - [ ] Implemented `transform_events/1` callback
  - [ ] Implemented `source_config/0` function

- [ ] **MetricsTracker Integration**
  - [ ] All jobs call `MetricsTracker.record_success/2`
  - [ ] All jobs call `MetricsTracker.record_failure/3` with error category
  - [ ] External IDs are consistent across job and metrics

- [ ] **Testing**
  - [ ] Client tests with HTTP mocking
  - [ ] Transformer tests with fixtures
  - [ ] Job integration tests
  - [ ] External ID format tests

- [ ] **Documentation**
  - [ ] Module @moduledoc explains purpose
  - [ ] Complex functions have @doc
  - [ ] Source added to `docs/ADDING_NEW_SOURCES.md`

- [ ] **Code Review**
  - [ ] Follows Elixir style guide
  - [ ] No hardcoded credentials
  - [ ] Rate limiting implemented
  - [ ] Error handling comprehensive

---

## 11. Architecture Decision Records (ADRs)

### ADR-001: Job Naming Convention

**Status:** Proposed
**Date:** 2025-01-23

**Decision:** Use `{JobType}Job` pattern for all job modules

**Context:**
- Legacy sources use inconsistent naming (`index_job` vs `index_page_job`)
- Module names should follow Elixir conventions (PascalCase)
- Need predictable naming for AI tooling and generators

**Consequences:**
- Requires renaming ~5 job files across sources
- Improves consistency and developer experience
- Enables better tooling and automation

**Alternatives Considered:**
1. Keep legacy names â†’ Rejected (inconsistent)
2. Use `Job{JobType}` suffix â†’ Rejected (non-idiomatic)

---

### ADR-002: External ID Format

**Status:** Proposed
**Date:** 2025-01-23

**Decision:** Use `{source}_{type}_{id}_{date}` format with hyphenated dates

**Context:**
- Current external IDs are inconsistent (URLs, underscored dates, etc.)
- Monitoring dashboards need parseable IDs
- Need unique identifiers for job execution tracking

**Consequences:**
- May require migration script for historical data
- All new sources must follow format
- Enables better filtering and searching in dashboards

**Alternatives Considered:**
1. Use UUIDs â†’ Rejected (not human-readable)
2. Use ISO 8601 timestamps â†’ Rejected (too verbose)
3. Use full URLs â†’ Rejected (not parseable)

---

### ADR-003: BaseJob Adoption

**Status:** Proposed
**Date:** 2025-01-23

**Decision:** All SyncJob files should use BaseJob behavior when applicable

**Context:**
- Reduces code duplication across sources
- Enforces consistent patterns (fetch â†’ transform â†’ process)
- Simplifies testing and maintenance

**Consequences:**
- Sources with complex orchestration may override `perform/1`
- Requires documentation of when to use vs. when to override
- Reduces boilerplate for simple sources

**Alternatives Considered:**
1. No shared behavior â†’ Rejected (too much duplication)
2. Require BaseJob for ALL jobs â†’ Rejected (too restrictive)
3. Create separate coordinators â†’ Rejected (adds complexity)

---

## 12. Zyte API Usage Guidelines (CRITICAL)

### When to Use Zyte (JavaScript Rendering)

**Zyte API costs ~$0.001 per request and adds 3-5 seconds of latency.** It should only be used as a **last resort** when plain HTTP cannot retrieve the necessary data.

### Decision Process (MANDATORY)

Before implementing Zyte for a new scraper, you MUST:

1. **Test Plain HTTP First**
   ```bash
   # Test with curl - does the data appear in the HTML?
   curl -s "https://example.com/event/123" -H "User-Agent: Mozilla/5.0" | head -1000
   ```

2. **Check for Server-Side Rendering (SSR)**
   - If event data appears in the initial HTML response â†’ **Use Plain HTTP**
   - If event data is loaded via JavaScript after page load â†’ **Consider Zyte**

3. **Look for Data in Meta Tags**
   ```html
   <!-- SSR sites often have data in meta tags - NO Zyte needed -->
   <meta property="og:title" content="Event Title">
   <meta property="og:description" content="Event description...">
   ```

4. **Check Network Tab**
   - Open browser DevTools â†’ Network tab â†’ Fetch/XHR
   - If event data comes from a JSON API endpoint â†’ **Call the API directly (no Zyte)**

### Zyte Is REQUIRED When:

- âœ… Site uses **client-side JavaScript framework** (React SPA, Vue SPA) that renders content dynamically
- âœ… Site has **Cloudflare anti-bot protection** that blocks automated requests
- âœ… Site returns **empty HTML** or **loading spinners** without JavaScript execution
- âœ… Content is loaded via **AJAX after initial page load** and no API endpoint is accessible

### Zyte Is NOT Required When:

- âŒ Site uses **Server-Side Rendering (SSR)** - data is in initial HTML
- âŒ Site has **meta tags** with event data (og:title, og:description, etc.)
- âŒ Site has a **public JSON API** that can be called directly
- âŒ Site is a **static HTML** website
- âŒ Site uses **PHP/Django/Rails** traditional backends (usually SSR)

### Implementation Pattern

**Plain HTTP (Preferred - FREE, ~200ms):**
```elixir
def fetch_page(url) do
  headers = [{"User-Agent", "Mozilla/5.0 (compatible; Eventasaurus/1.0)"}]

  case HTTPoison.get(url, headers, timeout: 30_000, recv_timeout: 30_000) do
    {:ok, %{status_code: 200, body: body}} ->
      {:ok, body}
    {:ok, %{status_code: status}} ->
      {:error, {:http_error, status}}
    {:error, reason} ->
      {:error, {:network_error, reason}}
  end
end
```

**Zyte Fallback (Only When Required - ~$0.001, 3-5s):**
```elixir
def fetch_page(url) do
  # Try plain HTTP first (free, fast)
  case fetch_with_plain_http(url) do
    {:ok, body} when byte_size(body) > 1000 ->
      # Validate we got real content, not a JS shell
      if contains_expected_data?(body) do
        {:ok, body}
      else
        # Content not in HTML, fall back to Zyte
        fetch_with_zyte(url)
      end

    {:error, _} ->
      # Network error or blocked, try Zyte
      fetch_with_zyte(url)
  end
end
```

### Cost Tracking

Every scraper using Zyte should log usage for cost monitoring:

```elixir
Logger.info("ðŸ’° Zyte API call: #{url} (estimated cost: $0.001)")
```

### Real-World Examples

**kupbilecik.pl (Poland)** - âŒ Does NOT require Zyte
- Uses Server-Side Rendering (SSR) for SEO
- All event data in initial HTML (og:title, semantic markup)
- Plain HTTP works perfectly (~200ms, FREE)

**bandsintown.com** - âœ… REQUIRES Zyte
- Uses client-side React with heavy JavaScript
- Has Cloudflare protection
- Plain HTTP returns empty content
- Zyte is necessary for browser rendering

### Checklist Before Using Zyte

- [ ] Tested plain HTTP with curl - does data appear in HTML?
- [ ] Checked meta tags for SSR data (og:title, og:description)
- [ ] Verified no public JSON API is available
- [ ] Confirmed site requires JavaScript execution
- [ ] Documented why Zyte is required in the client module
- [ ] Added cost logging for Zyte calls

**Remember:** Unnecessary Zyte usage wastes money and adds latency. Always prove it's necessary first!

---

## 13. Job Args Standards (CRITICAL)

Oban job arguments MUST follow a flat structure for consistency, debugging, and monitoring. **DO NOT nest metadata in sub-objects.**

### Standard Args Pattern

```elixir
# âœ… CORRECT - Flat structure with external_id at top level
%{
  "url" => "https://example.com/event/123",
  "source_id" => 456,
  "external_id" => "source_event_123_2024-01-15",
  "event_id" => "123",
  "city_id" => 789  # Optional, if applicable
}
```

```elixir
# âŒ WRONG - Nested metadata object
%{
  "source" => "source_name",
  "url" => "https://...",
  "event_metadata" => %{
    "event_id" => "123",
    "external_id_base" => "source_article_123"
  }
}
```

### Required Args by Job Type

#### SyncJob (Coordinator)
```elixir
%{
  "limit" => 100,       # Optional: max events to process
  "force" => false,     # Optional: force re-processing
  "city_id" => 123      # Optional: filter by city
}
```

#### IndexPageJob / ListingJob
```elixir
%{
  "source_id" => 123,
  "page" => 1,
  "city_id" => 456      # Optional
}
```

#### EventDetailJob
```elixir
%{
  "url" => "https://example.com/event/123",
  "source_id" => 123,
  "external_id" => "source_event_123_2024-01-15",  # REQUIRED at top level
  "event_id" => "123"   # Source-specific ID for logging
}
```

#### ShowtimeProcessJob / MovieDetailJob
```elixir
%{
  "source_id" => 123,
  "external_id" => "source_event_abc_2024-01-15",  # REQUIRED
  "showtime" => %{...},  # Raw data to process
  "{source}_film_id" => "xyz"  # Source-specific ID
}
```

### Why This Matters

1. **Oban Dashboard Visibility**: Args are shown in the dashboard UI - flat structure makes debugging easier
2. **Monitoring Queries**: Flat args allow simple SQL queries like `args->>'external_id'`
3. **MetricsTracker Integration**: `external_id` must be easily extractable for metrics
4. **Consistency**: All scrapers should look the same in monitoring tools

### External ID in Args

**CRITICAL**: The `external_id` MUST be passed in job args, NOT computed in the detail job.

```elixir
# âœ… CORRECT - SyncJob sets external_id when creating child jobs
EventDetailJob.new(%{
  "url" => url,
  "source_id" => source_id,
  "external_id" => "#{source_slug}_event_#{event_id}_#{Date.utc_today()}",
  "event_id" => event_id
})
|> Oban.insert()
```

```elixir
# âŒ WRONG - Detail job computes external_id from nested metadata
external_id = args["event_metadata"]["external_id_base"] || args["event_metadata"]["event_id"]
```

### Viewing Jobs in Oban Dashboard

With flat args, jobs appear cleanly in the dashboard:

```
Job #8659 EventDetailJob
Args:
  url: "https://www.kupbilecik.pl/imprezy/188122/..."
  source_id: 15
  external_id: "kupbilecik_event_188122_2024-01-15"
  event_id: "188122"
```

vs nested (harder to read):

```
Job #8659 EventDetailJob
Args:
  source: "kupbilecik"
  url: "https://..."
  event_metadata:
    event_id: "188122"
    external_id_base: "kupbilecik_article_188122"
```

### Checklist for Job Args

- [ ] `external_id` is at top level of args (not nested)
- [ ] `source_id` is included (not source slug string)
- [ ] No nested `event_metadata` or `metadata` objects
- [ ] Job-specific IDs use format `{source}_{type}_id` (e.g., `cinema_city_film_id`)
- [ ] URLs are complete (not relative paths)

---

## 14. Deduplication Strategies

### Overview

Deduplication is critical for event aggregation. When scraping from multiple sources, the same real-world event may appear from different ticketing platforms (Ticketmaster, Bandsintown, local sites). The deduplication system prevents duplicate events while respecting source priorities.

### Two-Phase Deduplication Architecture

All DedupHandlers follow a two-phase approach:

**Phase 1: Same-Source Deduplication**
- Check if THIS source already imported the event via `external_id` match
- Fast lookup using `BaseDedupHandler.find_by_external_id/2`
- Prevents re-importing the same event on subsequent syncs

**Phase 2: Cross-Source Fuzzy Matching**
- Check if a HIGHER-PRIORITY source already has this event
- Uses fuzzy matching based on strategy type (see below)
- Respects source priority (Ticketmaster > Bandsintown > local sources)
- Only defers to sources with higher priority scores

### Strategy Types

There are **four distinct deduplication strategies** used across sources, chosen based on the type of content each source provides:

#### Strategy 1: Performer-Based Deduplication

**Used By:** Bandsintown, Ticketmaster, Kupbilecik

**When to Use:** Music events, concerts, festivals where **performer/artist name** is the primary identifier.

**Rationale:** When the same performer plays at the same venue on the same date, it's almost certainly the same event, regardless of title variations ("Taylor Swift", "Taylor Swift Concert", "Taylor Swift | The Eras Tour").

**Confidence Scoring:**
| Signal | Weight | Notes |
|--------|--------|-------|
| Performer Name Match | 40% | Normalized, case-insensitive |
| Date Match | 25% | Same calendar date |
| Venue Name Match | 20% | Fuzzy string matching |
| GPS Proximity | 15% | Within 100m radius |

**GPS Radius:** 100 meters (tight radius for concert venues)

**Implementation Pattern:**
```elixir
defmodule MySource.DedupHandler do
  def check_duplicate(event_data, source) do
    # Phase 1: Same-source via external_id
    case BaseDedupHandler.find_by_external_id(event_data[:external_id], source.id) do
      %Event{} = existing -> {:duplicate, existing}
      nil -> check_fuzzy_duplicate(event_data, source)
    end
  end

  defp check_fuzzy_duplicate(event_data, source) do
    performer_name = extract_performer_name(event_data)
    date = event_data[:starts_at]
    {lat, lng} = get_coordinates(event_data)

    matches = BaseDedupHandler.find_events_by_date_and_proximity(
      date, lat, lng, proximity_meters: 100
    )

    performer_matches = Enum.filter(matches, fn %{event: event} ->
      similar_performer?(performer_name, event.title)
    end)

    # Filter for higher-priority sources only
    higher_priority = BaseDedupHandler.filter_higher_priority_matches(performer_matches, source)

    case higher_priority do
      [] -> {:unique, event_data}
      [match | _] ->
        confidence = calculate_match_confidence(event_data, match.event)
        if BaseDedupHandler.should_defer_to_match?(match, source, confidence) do
          {:duplicate, match.event}
        else
          {:unique, event_data}
        end
    end
  end
end
```

---

#### Strategy 2: Title + Venue + Date Deduplication

**Used By:** Cinema City, Karnet, Repertuary, Resident Advisor

**When to Use:** Movies, cultural events, general listings where **event title** is the primary identifier.

**Rationale:** For movie screenings or cultural events, the title is consistent across sources ("Dune Part Two" will be called that everywhere). Combined with venue and date, this provides reliable matching.

**Confidence Scoring:**
| Signal | Weight | Notes |
|--------|--------|-------|
| Title Match | 40% | Normalized, case-insensitive |
| Date Match | 30% | Same calendar date |
| GPS Proximity | 30% | Within 500m radius |

**GPS Radius:** 500 meters (larger radius for general venues/cinemas)

**Implementation Pattern:**
```elixir
defp check_fuzzy_duplicate(event_data, source) do
  title = normalize_title(event_data[:title])
  date = event_data[:starts_at]
  {lat, lng} = get_coordinates(event_data)

  matches = BaseDedupHandler.find_events_by_date_and_proximity(
    date, lat, lng, proximity_meters: 500
  )

  title_matches = Enum.filter(matches, fn %{event: event} ->
    similar_title?(title, event.title)
  end)

  # Continue with priority filtering...
end
```

---

#### Strategy 3: Venue + Recurrence Deduplication

**Used By:** PubQuiz

**When to Use:** Recurring events at fixed venues (trivia nights, weekly meetups, regular club nights).

**Rationale:** For recurring events, the venue itself is the key identifier. "Tuesday Trivia at O'Malley's" is the same event every Tuesday, regardless of slight title variations.

**Confidence Scoring:**
| Signal | Weight | Notes |
|--------|--------|-------|
| Venue Name Match | 50% | Primary identifier |
| GPS Proximity | 40% | Within 50m radius |
| Title/Theme Match | 10% | Secondary confirmation |

**GPS Radius:** 50 meters (very tight radius for recurring venue events)

**Implementation Pattern:**
```elixir
defp check_fuzzy_duplicate(event_data, source) do
  venue_name = event_data[:venue_data][:name]
  date = event_data[:starts_at]
  {lat, lng} = get_coordinates(event_data)

  # Very tight GPS radius for recurring venue events
  matches = BaseDedupHandler.find_events_by_date_and_proximity(
    date, lat, lng, proximity_meters: 50
  )

  venue_matches = Enum.filter(matches, fn %{event: event} ->
    same_venue?(venue_name, event.venue.name)
  end)

  # Check for same day-of-week pattern (recurring events)
  # Continue with priority filtering...
end
```

---

#### Strategy 4: Umbrella Event Detection

**Used By:** Resident Advisor (special handling)

**When to Use:** Festival or multi-venue events where a parent event encompasses multiple sub-events.

**Rationale:** Some events like "ADE Festival" occur across many venues. These "umbrella events" need special handling to avoid treating each venue's listing as a separate event.

**Detection Signals:**
- Venue name contains "Various" or is generic ("Multiple Locations")
- Event spans multiple days
- Title matches known festival patterns

**Implementation Note:**
```elixir
defp is_umbrella_event?(event_data) do
  venue_name = event_data[:venue_data][:name] || ""

  String.contains?(venue_name, "Various") ||
    String.contains?(venue_name, "Multiple") ||
    event_data[:is_multi_day] == true
end
```

---

### Decision Matrix: Choosing a Strategy

| Source Type | Has Performer Data? | Content Type | Recommended Strategy |
|-------------|--------------------|--------------|--------------------|
| Ticketing (Bandsintown, TM) | âœ… Yes | Concerts | **Performer-Based** |
| Polish Ticketing (Kupbilecik) | âœ… Yes | Concerts/Shows | **Performer-Based** |
| Cinema (Cinema City) | âŒ No | Movie Screenings | **Title + Venue + Date** |
| Cultural (Karnet, Repertuary) | âŒ No | Events/Shows | **Title + Venue + Date** |
| Music (Resident Advisor) | Sometimes | Club Events | **Title + Venue + Date** |
| Trivia/Recurring (PubQuiz) | âŒ No | Weekly Events | **Venue + Recurrence** |

### Source Priority System

Higher-priority sources "win" when duplicates are detected. Lower-priority sources defer to existing higher-priority events.

| Priority | Source | Type |
|----------|--------|------|
| 90 | Ticketmaster | Major ticketing |
| 80 | Bandsintown | Artist-focused |
| 75 | Resident Advisor | Electronic music |
| 70 | Kupbilecik | Polish ticketing |
| 60 | Karnet | Cultural events |
| 50 | Cinema City | Movie screenings |
| 40 | PubQuiz | Trivia/recurring |
| 30 | Waw4Free | Free events |

**Priority Logic:**
- When Kupbilecik (70) finds a matching Bandsintown event (80), it defers â†’ `{:duplicate, bandsintown_event}`
- When Kupbilecik (70) finds a matching Waw4Free event (30), it does NOT defer â†’ `{:unique, event_data}`

### Confidence Thresholds

All strategies use a confidence threshold of **0.8 (80%)** before declaring a match:

```elixir
@confidence_threshold 0.8

def should_defer_to_match?(match, source, confidence) do
  confidence >= @confidence_threshold &&
    match.source.priority > source.priority
end
```

### Creating a New DedupHandler

**Step 1:** Determine which strategy fits your source type (see Decision Matrix)

**Step 2:** Create the handler module:
```
lib/eventasaurus_discovery/sources/{source_slug}/dedup_handler.ex
```

**Step 3:** Implement the required functions:
```elixir
defmodule EventasaurusDiscovery.Sources.MySource.DedupHandler do
  @moduledoc """
  Deduplication handler for MySource.

  Strategy: [Performer-Based | Title + Venue + Date | Venue + Recurrence]
  GPS Radius: [50m | 100m | 500m]
  """

  alias EventasaurusDiscovery.Sources.BaseDedupHandler

  @doc "Check if event already exists"
  def check_duplicate(event_data, source) do
    # Phase 1: Same-source check
    # Phase 2: Cross-source fuzzy match
  end

  @doc "Validate event quality before processing"
  def validate_event_quality(event_data) do
    # Required fields, date sanity checks
  end
end
```

**Step 4:** Add tests:
```
test/eventasaurus_discovery/sources/{source_slug}/dedup_handler_test.exs
```

### Testing Deduplication

**Unit Tests (no database):**
```elixir
describe "validate_event_quality/1" do
  test "accepts valid event data" do
    event = %{title: "Concert", external_id: "src_123", starts_at: future_date()}
    assert {:ok, ^event} = DedupHandler.validate_event_quality(event)
  end

  test "rejects past events" do
    event = %{title: "Concert", external_id: "src_123", starts_at: past_date()}
    assert {:error, reason} = DedupHandler.validate_event_quality(event)
    assert reason =~ "past"
  end
end
```

**Integration Tests (with database):**
```elixir
describe "check_duplicate/2" do
  test "detects same-source duplicate" do
    existing = insert(:event, external_id: "src_event_123_2025-01-01")
    event_data = %{external_id: "src_event_123_2025-01-01", ...}

    assert {:duplicate, ^existing} = DedupHandler.check_duplicate(event_data, source)
  end

  test "defers to higher-priority source" do
    # Create event from Bandsintown (priority 80)
    bandsintown_event = insert(:event, source: bandsintown_source, ...)

    # Try to import same event via Kupbilecik (priority 70)
    event_data = similar_event_data(bandsintown_event)

    assert {:duplicate, ^bandsintown_event} = DedupHandler.check_duplicate(event_data, kupbilecik_source)
  end
end
```

### Checklist for Deduplication Implementation

- [ ] **Strategy Selection**
  - [ ] Identified correct strategy for source type
  - [ ] Documented strategy choice in module @moduledoc

- [ ] **Phase 1: Same-Source**
  - [ ] Uses `BaseDedupHandler.find_by_external_id/2`
  - [ ] Returns `{:duplicate, existing}` on match

- [ ] **Phase 2: Cross-Source**
  - [ ] Correct GPS radius for strategy (50m/100m/500m)
  - [ ] Proper confidence weights implemented
  - [ ] Uses `filter_higher_priority_matches/2`
  - [ ] Respects confidence threshold (0.8)

- [ ] **Quality Validation**
  - [ ] Required fields checked (title, external_id, starts_at)
  - [ ] Date sanity (not past, not >2 years future)
  - [ ] Returns `{:ok, event_data}` or `{:error, reason}`

- [ ] **Testing**
  - [ ] Unit tests for validation logic
  - [ ] Integration tests for duplicate detection
  - [ ] Tests for priority deferral

---

## Related Documentation

- [ADDING_NEW_SOURCES.md](./ADDING_NEW_SOURCES.md) - Step-by-step guide for new sources
- [SCRAPER_MANIFESTO.md](./SCRAPER_MANIFESTO.md) - High-level scraper philosophy
- [scraper-monitoring-guide.md](./scraper-monitoring-guide.md) - Monitoring and metrics
- [SCRAPER_QUALITY_GUIDELINES.md](./SCRAPER_QUALITY_GUIDELINES.md) - Code quality standards

---

**Questions or Feedback?** Open a GitHub issue with label `documentation` or `source-implementation`.
